{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795d67a2-8b02-460d-ac78-49b953a25411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime, time\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs(\"youtube/data/csv\", exist_ok=True)\n",
    "\n",
    "def extract_country_from_filename(path):\n",
    "    name = os.path.basename(path)\n",
    "    name_no_ext = os.path.splitext(name)[0]  # remove extension\n",
    "    \n",
    "    # Try matching first 2 uppercase letters at the start\n",
    "    m = re.match(r'^([A-Za-z]{2})', name_no_ext)\n",
    "    if m:\n",
    "        return m.group(1).upper()\n",
    "    \n",
    "    # Otherwise, look for any 2 letter part in the name\n",
    "    parts = re.split(r'[^A-Za-z]+', name_no_ext)\n",
    "    for p in parts:\n",
    "        if p and len(p) < 3:\n",
    "            return p.upper()\n",
    "    \n",
    "    # fallback to generic \"UNKNOWN\"\n",
    "    #return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def parse_publish_time(s):\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    if isinstance(s, (datetime,)):\n",
    "        return s.time()\n",
    "    s = str(s)\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(s.replace('Z', '+00:00'))\n",
    "        return dt.time()\n",
    "    except Exception:\n",
    "        pass\n",
    "    for fmt in (\"%H:%M:%S\", \"%H:%M\"):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt).time()\n",
    "        except Exception:\n",
    "            pass\n",
    "    m = re.search(r'(\\d{1,2}:\\d{2})', s)\n",
    "    if m:\n",
    "        try:\n",
    "            return datetime.strptime(m.group(1), \"%H:%M\").time()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def bucket_time_10min(t: time):\n",
    "    if t is None:\n",
    "        return None\n",
    "    minutes = t.hour * 60 + t.minute\n",
    "    bucket_start = (minutes // 10) * 10\n",
    "    start_h = bucket_start // 60\n",
    "    start_m = bucket_start % 60\n",
    "    end_minutes = bucket_start + 10\n",
    "    end_h = end_minutes // 60\n",
    "    end_m = end_minutes % 60\n",
    "    return f\"{start_h:02d}:{start_m:02d}-{end_h:02d}:{end_m:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0839d-fd80-4770-bb43-809792208853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Create a single dataframe with the concatenation of all input csv files, adding a column called country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a83fa99-b9ba-46dd-83f9-09ca3d143a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Utente\\\\youtube\\\\data\\\\csv\\\\CAvideos.csv',\n",
       " 'C:\\\\Users\\\\Utente\\\\youtube\\\\data\\\\csv\\\\DEvideos.csv',\n",
       " 'C:\\\\Users\\\\Utente\\\\youtube\\\\data\\\\csv\\\\FRvideos.csv',\n",
       " 'C:\\\\Users\\\\Utente\\\\youtube\\\\data\\\\csv\\\\GBvideos.csv',\n",
       " 'C:\\\\Users\\\\Utente\\\\youtube\\\\data\\\\csv\\\\INvideos.csv',\n",
       " 'C:\\\\Users\\\\Utente\\\\youtube\\\\data\\\\csv\\\\JPvideos.csv',\n",
       " 'C:\\\\Users\\\\Utente\\\\youtube\\\\data\\\\csv\\\\KRvideos.csv',\n",
       " 'C:\\\\Users\\\\Utente\\\\youtube\\\\data\\\\csv\\\\MXvideos.csv',\n",
       " 'C:\\\\Users\\\\Utente\\\\youtube\\\\data\\\\csv\\\\RUvideos.csv',\n",
       " 'C:\\\\Users\\\\Utente\\\\youtube\\\\data\\\\csv\\\\USvideos.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dir = r'C:\\Users\\Utente\\youtube\\data\\csv'\n",
    "all_csv_paths = sorted(glob.glob(os.path.join(csv_dir, '*.csv')))\n",
    "#print(f\"Number of CSV files: {len(all_csv_paths)}\")\n",
    "all_csv_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8d584f-7548-4c5a-9567-3aa8c8ad4c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      video_id trending_date  \\\n",
      "0  n1WpP7iowLc      17.14.11   \n",
      "1  0dBIkQ4Mz1M      17.14.11   \n",
      "2  5qpjK5DgCt4      17.14.11   \n",
      "3  d380meD0W0M      17.14.11   \n",
      "4  2Vv-BfVoq4g      17.14.11   \n",
      "\n",
      "                                               title channel_title  \\\n",
      "0         Eminem - Walk On Water (Audio) ft. BeyoncÃ©    EminemVEVO   \n",
      "1                      PLUSH - Bad Unboxing Fan Mail     iDubbbzTV   \n",
      "2  Racist Superman | Rudy Mancuso, King Bach & Le...  Rudy Mancuso   \n",
      "3                           I Dare You: GOING BALD!?      nigahiga   \n",
      "4        Ed Sheeran - Perfect (Official Music Video)    Ed Sheeran   \n",
      "\n",
      "   category_id              publish_time  \\\n",
      "0           10  2017-11-10T17:00:03.000Z   \n",
      "1           23  2017-11-13T17:00:00.000Z   \n",
      "2           23  2017-11-12T19:05:24.000Z   \n",
      "3           24  2017-11-12T18:01:41.000Z   \n",
      "4           10  2017-11-09T11:04:14.000Z   \n",
      "\n",
      "                                                tags     views    likes  \\\n",
      "0  Eminem|\"Walk\"|\"On\"|\"Water\"|\"Aftermath/Shady/In...  17158579   787425   \n",
      "1  plush|\"bad unboxing\"|\"unboxing\"|\"fan mail\"|\"id...   1014651   127794   \n",
      "2  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...   3191434   146035   \n",
      "3  ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...   2095828   132239   \n",
      "4  edsheeran|\"ed sheeran\"|\"acoustic\"|\"live\"|\"cove...  33523622  1634130   \n",
      "\n",
      "   dislikes  comment_count                                  thumbnail_link  \\\n",
      "0     43420         125882  https://i.ytimg.com/vi/n1WpP7iowLc/default.jpg   \n",
      "1      1688          13030  https://i.ytimg.com/vi/0dBIkQ4Mz1M/default.jpg   \n",
      "2      5339           8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
      "3      1989          17518  https://i.ytimg.com/vi/d380meD0W0M/default.jpg   \n",
      "4     21082          85067  https://i.ytimg.com/vi/2Vv-BfVoq4g/default.jpg   \n",
      "\n",
      "   comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
      "0              False             False                   False   \n",
      "1              False             False                   False   \n",
      "2              False             False                   False   \n",
      "3              False             False                   False   \n",
      "4              False             False                   False   \n",
      "\n",
      "                                         description country  \n",
      "0  Eminem's new track Walk on Water ft. BeyoncÃ© i...      CA  \n",
      "1  STill got a lot of packages. Probably will las...      CA  \n",
      "2  WATCH MY PREVIOUS VIDEO â–¶ \\n\\nSUBSCRIBE â–º http...      CA  \n",
      "3  I know it's been a while since we did this sho...      CA  \n",
      "4  ðŸŽ§: https://ad.gt/yt-perfect\\nðŸ’°: https://atlant...      CA  \n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for path in all_csv_paths:\n",
    "    try:\n",
    "        # Try reading with 'utf-8', if fails, fall back to 'latin1'\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(path, encoding='latin1')\n",
    "        \n",
    "        df['country'] = extract_country_from_filename(path)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "\n",
    "# Concatenate all successfully read CSVs\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679724c-a3f9-453b-acdf-d798e8d2df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.Cluster the publish time into 10-minute intervals (e.g. from 02:20 to 02:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131c58f7-9261-43b4-8bc2-7c879caeedc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               publish_time publish_time_parsed publish_time_bucket\n",
      "0  2017-11-10T17:00:03.000Z            17:00:03         17:00-17:10\n",
      "1  2017-11-13T17:00:00.000Z            17:00:00         17:00-17:10\n",
      "2  2017-11-12T19:05:24.000Z            19:05:24         19:00-19:10\n",
      "3  2017-11-12T18:01:41.000Z            18:01:41         18:00-18:10\n",
      "4  2017-11-09T11:04:14.000Z            11:04:14         11:00-11:10\n"
     ]
    }
   ],
   "source": [
    "final_df['publish_time_parsed'] = final_df['publish_time'].apply(parse_publish_time)\n",
    "final_df['publish_time_bucket'] = final_df['publish_time_parsed'].apply(bucket_time_10min)\n",
    "print(final_df[['publish_time', 'publish_time_parsed', 'publish_time_bucket']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb41fa-417e-48ad-a541-c5bd689f1716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.For each interval, determine the number of videos, average number of likes and of dislikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1144f91e-ac44-473b-851d-4387187ad77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  publish_time_bucket  num_videos     avg_likes  avg_dislikes\n",
      "0         00:00-00:10        2913  60951.483350   3787.232750\n",
      "1         00:10-00:20        1522  22553.870565   1437.457293\n",
      "2         00:20-00:30        1248  21258.370192   1066.330128\n",
      "3         00:30-00:40        1625  36604.352000    949.439385\n",
      "4         00:40-00:50        1283  41770.614186   1889.012471\n"
     ]
    }
   ],
   "source": [
    "interval_stats = final_df.groupby('publish_time_bucket').agg(\n",
    "    num_videos=('video_id', 'count'),\n",
    "    avg_likes=('likes', 'mean'),\n",
    "    avg_dislikes=('dislikes', 'mean')\n",
    ").reset_index()\n",
    "print(interval_stats.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a91a6b-2584-4895-891e-3a7bbd8fdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.For each tag, determine the number of videos. Notice that tags contains a string with several tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba65b89-dce8-441c-ab36-3d5fc2d15c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tags: 865696\n",
      "\n",
      "Top 10 most used tags with the numbers of repeat:\n",
      "                          tag  num_tag_used\n",
      "0                      Eminem           462\n",
      "1                        Walk           139\n",
      "2                          On            96\n",
      "3                       Water           109\n",
      "4  Aftermath/Shady/Interscope           127\n",
      "5                         Rap          2299\n",
      "6                       plush            19\n",
      "7                bad unboxing            20\n",
      "8                    unboxing          1260\n",
      "9                    fan mail            39\n"
     ]
    }
   ],
   "source": [
    "all_tags = []\n",
    "\n",
    "for tags_string in final_df['tags']:\n",
    "    if pd.notna(tags_string):  # Check if not NaN\n",
    "        # Split by pipe and clean each tag\n",
    "        tags_list = str(tags_string).split('|')\n",
    "        # Remove quotes and whitespace from each tag\n",
    "        cleaned_tags = [tag.strip().strip('\"') for tag in tags_list if tag.strip()]\n",
    "        all_tags.extend(cleaned_tags) #extend: add tags one by one \n",
    "\n",
    "# Count occurrences of each tag\n",
    "tag_counts = Counter(all_tags)\n",
    "#converting to a dataframe to print it\n",
    "tag_stats = pd.DataFrame(tag_counts.items(), columns=['tag', 'num_tag_used'])\n",
    "print(f\"Total unique tags: {len(tag_counts)}\")\n",
    "print(\"\\nTop 10 most used tags with the numbers of repeat:\")\n",
    "print(tag_stats.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a9c82-d367-485b-b36a-05319024f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Find the tags with the largest number of videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3bfb89-020f-4ed5-8136-f3d38f753818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tag  num_tag_used\n",
      "822         [none]         37698\n",
      "51           funny         15075\n",
      "50          comedy         12356\n",
      "8292          2018         11402\n",
      "726           news          6415\n",
      "153          music          5918\n",
      "1189          2017          5698\n",
      "472          video          5640\n",
      "327          humor          5058\n",
      "310     television          4174\n",
      "680           show          4158\n",
      "373         review          4068\n",
      "112           vlog          3979\n",
      "2125           Pop          3963\n",
      "3235     interview          3861\n",
      "57            live          3809\n",
      "531           food          3688\n",
      "1302  funny videos          3610\n",
      "862       comedian          3601\n",
      "456             tv          3453\n"
     ]
    }
   ],
   "source": [
    "tag_stats = tag_stats.sort_values('num_tag_used', ascending=False)\n",
    "print(tag_stats.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a9c2c4-4ddb-4e0d-a088-8220d558d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. For each (tag, country) pair, compute average ratio likes/dislikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e2358-b174-4725-804f-d5917b22e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df.copy()\n",
    "\n",
    "# 1. Clean and split tags into lists \n",
    "df['tag_list'] = df['tags'].apply(\n",
    "    lambda x: [t.strip().strip('\"') for t in str(x).split('|') if t.strip()] \n",
    "    if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "# 2. Explode so each tag has its own row\n",
    "df_exploded = df.explode('tag_list')\n",
    "\n",
    "# 3. Compute ratio likes/dislikes\n",
    "# Avoid division by zero â†’ replace dislikes = 0 with 1 to calculate number of likes for those videos\n",
    "df_exploded['dislikes'] = df_exploded['dislikes'].replace(0, 1)\n",
    "\n",
    "df_exploded['like/dislike_ratio'] = df_exploded['likes'] / df_exploded['dislikes']\n",
    "\n",
    "# 4. Group by tag and country and compute the average ratio\n",
    "avg_ratio = (\n",
    "    df_exploded.groupby(['tag_list', 'country'])['like/dislike_ratio']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'tag_list': 'tag', 'like/dislike_ratio': 'avg_like/dislike_ratio'})\n",
    ")\n",
    "\n",
    "avg_ratio.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
